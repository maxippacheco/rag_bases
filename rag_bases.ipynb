{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_vwyByCQ635",
        "outputId": "96ea403b-ed96-4e30-ce48-da650323b255"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: openai 1.60.2\n",
            "Uninstalling openai-1.60.2:\n",
            "  Successfully uninstalled openai-1.60.2\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall openai -y"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uKRH3E2kQ_CX",
        "outputId": "fb11235c-8fbf-43f7-d654-bf85dc90ef85"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "hLgiSpn2TWLH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f = open(\"drive/MyDrive/api_key.txt\", \"r\")\n",
        "API_KEY = f.readline().strip()\n",
        "f.close()"
      ],
      "metadata": {
        "id": "3bobkNbuReAp"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "os.environ['OPENAI_API_KEY'] = API_KEY\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
      ],
      "metadata": {
        "id": "4QhZPNqyTa_f"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "from openai import OpenAI\n",
        "import time\n",
        "client = OpenAI()\n",
        "gptmodel=\"gpt-4o\"\n",
        "start_time = time.time() # start timing before the request"
      ],
      "metadata": {
        "id": "GUdkAGRsUm3P"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def call_llm_with_full_text(itext: str) -> str:\n",
        "  # Join all lines to form a single string\n",
        "  text_input = '\\n'.join(itext)\n",
        "  prompt = f\"Please elaborate on the following content:\\n{text_input}\"\n",
        "\n",
        "  try:\n",
        "    response = client.chat.completions.create(\n",
        "        model=gptmodel,\n",
        "        messages = [\n",
        "            {\n",
        "              \"role\": \"system\",\n",
        "              \"content\": \"You are an expert Natural Language Processing exercise expert.\"\n",
        "            },\n",
        "            {\n",
        "              \"role\": \"assistant\",\n",
        "              \"content\": \"1. You can explain read the input and answer in detail\"\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": prompt\n",
        "            }\n",
        "        ],\n",
        "        temperature=0.1 # low = more precise\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()\n",
        "  except Exception as e:\n",
        "    return str(e)"
      ],
      "metadata": {
        "id": "vwpofCuzXDII"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "\n",
        "def print_formatted_response(response):\n",
        "  # width\n",
        "  wrapper = textwrap.TextWrapper(width=80) # 80 cols wide\n",
        "  wrapped_text = wrapper.fill(text=response)\n",
        "  # format\n",
        "  print(\"Response:\")\n",
        "  print(\"---------------\")\n",
        "  print(wrapped_text)\n",
        "  print(\"---------------\\n\")"
      ],
      "metadata": {
        "id": "D3F61B8UXP44"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The generator is now ready to be called when we need it. Due to the probabilistic nature\n",
        "of generative AI models, it might produce different outputs each time we call it"
      ],
      "metadata": {
        "id": "HbKKQSUvYuif"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The data\n",
        "# We assume that the data has been collected, cleaned and split into sentences.\n",
        "db_records = [\n",
        "    \"Retrieval Augmented Generation (RAG) represents a sophisticated hybrid approach in the field of artificial intelligence, particularly within the realm of natural language processing (NLP).\",\n",
        "    \"It innovatively combines the capabilities of neural network-based language models with retrieval systems to enhance the generation of text, making it more accurate, informative, and contextually relevant.\",\n",
        "    \"This methodology leverages the strengths of both generative and retrieval architectures to tackle complex tasks that require not only linguistic fluency but also factual correctness and depth of knowledge.\",\n",
        "    \"At the core of Retrieval Augmented Generation (RAG) is a generative model, typically a transformer-based neural network, similar to those used in models like GPT (Generative Pre-trained Transformer) or BERT (Bidirectional Encoder Representations from Transformers).\",\n",
        "    \"This component is responsible for producing coherent and contextually appropriate language outputs based on a mixture of input prompts and additional information fetched by the retrieval component.\",\n",
        "    \"Complementing the language model is the retrieval system, which is usually built on a database of documents or a corpus of texts.\",\n",
        "    \"This system uses techniques from information retrieval to find and fetch documents that are relevant to the input query or prompt.\",\n",
        "    \"The mechanism of relevance determination can range from simple keyword matching to more complex semantic search algorithms which interpret the meaning behind the query to find the best matches.\",\n",
        "    \"This component merges the outputs from the language model and the retrieval system.\",\n",
        "    \"It effectively synthesizes the raw data fetched by the retrieval system into the generative process of the language model.\",\n",
        "    \"The integrator ensures that the information from the retrieval system is seamlessly incorporated into the final text output, enhancing the model's ability to generate responses that are not only fluent and grammatically correct but also rich in factual details and context-specific nuances.\",\n",
        "    \"When a query or prompt is received, the system first processes it to understand the requirement or the context.\",\n",
        "    \"Based on the processed query, the retrieval system searches through its database to find relevant documents or information snippets.\",\n",
        "    \"This retrieval is guided by the similarity of content in the documents to the query, which can be determined through various techniques like vector embeddings or semantic similarity measures.\",\n",
        "    \"The retrieved documents are then fed into the language model.\",\n",
        "    \"In some implementations, this integration happens at the token level, where the model can access and incorporate specific pieces of information from the retrieved texts dynamically as it generates each part of the response.\",\n",
        "    \"The language model, now augmented with direct access to retrieved information, generates a response.\",\n",
        "    \"This response is not only influenced by the training of the model but also by the specific facts and details contained in the retrieved documents, making it more tailored and accurate.\",\n",
        "    \"By directly incorporating information from external sources, Retrieval Augmented Generation (RAG) models can produce responses that are more factual and relevant to the given query.\",\n",
        "    \"This is particularly useful in domains like medical advice, technical support, and other areas where precision and up-to-date knowledge are crucial.\",\n",
        "    \"Retrieval Augmented Generation (RAG) systems can dynamically adapt to new information since they retrieve data in real-time from their databases.\",\n",
        "    \"This allows them to remain current with the latest knowledge and trends without needing frequent retraining.\",\n",
        "    \"With access to a wide range of documents, Retrieval Augmented Generation (RAG) systems can provide detailed and nuanced answers that a standalone language model might not be capable of generating based solely on its pre-trained knowledge.\",\n",
        "    \"While Retrieval Augmented Generation (RAG) offers substantial benefits, it also comes with its challenges.\",\n",
        "    \"These include the complexity of integrating retrieval and generation systems, the computational overhead associated with real-time data retrieval, and the need for maintaining a large, up-to-date, and high-quality database of retrievable texts.\",\n",
        "    \"Furthermore, ensuring the relevance and accuracy of the retrieved information remains a significant challenge, as does managing the potential for introducing biases or errors from the external sources.\",\n",
        "    \"In summary, Retrieval Augmented Generation represents a significant advancement in the field of artificial intelligence, merging the best of retrieval-based and generative technologies to create systems that not only understand and generate natural language but also deeply comprehend and utilize the vast amounts of information available in textual form.\",\n",
        "    \"A RAG vector store is a database or dataset that contains vectorized data points.\"\n",
        "]"
      ],
      "metadata": {
        "id": "DG7fnkNFYzzv"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "paragraph = ' '.join(db_records)\n",
        "wrapped_text = textwrap.fill(paragraph, width=80)\n",
        "print(wrapped_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XMVgYTLJZT5S",
        "outputId": "0a36f7a8-bc05-45fd-d40d-f41f00003b26"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieval Augmented Generation (RAG) represents a sophisticated hybrid approach\n",
            "in the field of artificial intelligence, particularly within the realm of\n",
            "natural language processing (NLP). It innovatively combines the capabilities of\n",
            "neural network-based language models with retrieval systems to enhance the\n",
            "generation of text, making it more accurate, informative, and contextually\n",
            "relevant. This methodology leverages the strengths of both generative and\n",
            "retrieval architectures to tackle complex tasks that require not only linguistic\n",
            "fluency but also factual correctness and depth of knowledge. At the core of\n",
            "Retrieval Augmented Generation (RAG) is a generative model, typically a\n",
            "transformer-based neural network, similar to those used in models like GPT\n",
            "(Generative Pre-trained Transformer) or BERT (Bidirectional Encoder\n",
            "Representations from Transformers). This component is responsible for producing\n",
            "coherent and contextually appropriate language outputs based on a mixture of\n",
            "input prompts and additional information fetched by the retrieval component.\n",
            "Complementing the language model is the retrieval system, which is usually built\n",
            "on a database of documents or a corpus of texts. This system uses techniques\n",
            "from information retrieval to find and fetch documents that are relevant to the\n",
            "input query or prompt. The mechanism of relevance determination can range from\n",
            "simple keyword matching to more complex semantic search algorithms which\n",
            "interpret the meaning behind the query to find the best matches. This component\n",
            "merges the outputs from the language model and the retrieval system. It\n",
            "effectively synthesizes the raw data fetched by the retrieval system into the\n",
            "generative process of the language model. The integrator ensures that the\n",
            "information from the retrieval system is seamlessly incorporated into the final\n",
            "text output, enhancing the model's ability to generate responses that are not\n",
            "only fluent and grammatically correct but also rich in factual details and\n",
            "context-specific nuances. When a query or prompt is received, the system first\n",
            "processes it to understand the requirement or the context. Based on the\n",
            "processed query, the retrieval system searches through its database to find\n",
            "relevant documents or information snippets. This retrieval is guided by the\n",
            "similarity of content in the documents to the query, which can be determined\n",
            "through various techniques like vector embeddings or semantic similarity\n",
            "measures. The retrieved documents are then fed into the language model. In some\n",
            "implementations, this integration happens at the token level, where the model\n",
            "can access and incorporate specific pieces of information from the retrieved\n",
            "texts dynamically as it generates each part of the response. The language model,\n",
            "now augmented with direct access to retrieved information, generates a response.\n",
            "This response is not only influenced by the training of the model but also by\n",
            "the specific facts and details contained in the retrieved documents, making it\n",
            "more tailored and accurate. By directly incorporating information from external\n",
            "sources, Retrieval Augmented Generation (RAG) models can produce responses that\n",
            "are more factual and relevant to the given query. This is particularly useful in\n",
            "domains like medical advice, technical support, and other areas where precision\n",
            "and up-to-date knowledge are crucial. Retrieval Augmented Generation (RAG)\n",
            "systems can dynamically adapt to new information since they retrieve data in\n",
            "real-time from their databases. This allows them to remain current with the\n",
            "latest knowledge and trends without needing frequent retraining. With access to\n",
            "a wide range of documents, Retrieval Augmented Generation (RAG) systems can\n",
            "provide detailed and nuanced answers that a standalone language model might not\n",
            "be capable of generating based solely on its pre-trained knowledge. While\n",
            "Retrieval Augmented Generation (RAG) offers substantial benefits, it also comes\n",
            "with its challenges. These include the complexity of integrating retrieval and\n",
            "generation systems, the computational overhead associated with real-time data\n",
            "retrieval, and the need for maintaining a large, up-to-date, and high-quality\n",
            "database of retrievable texts. Furthermore, ensuring the relevance and accuracy\n",
            "of the retrieved information remains a significant challenge, as does managing\n",
            "the potential for introducing biases or errors from the external sources. In\n",
            "summary, Retrieval Augmented Generation represents a significant advancement in\n",
            "the field of artificial intelligence, merging the best of retrieval-based and\n",
            "generative technologies to create systems that not only understand and generate\n",
            "natural language but also deeply comprehend and utilize the vast amounts of\n",
            "information available in textual form. A RAG vector store is a database or\n",
            "dataset that contains vectorized data points.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The query\n",
        "# the query itself is simply user input or automated input from another AI agent\n",
        "query = \"define a rag store\""
      ],
      "metadata": {
        "id": "QWFvOh29ZhYI"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_response = call_llm_with_full_text(query)\n",
        "print_formatted_response(llm_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bctxu34OZ2XX",
        "outputId": "5fe30e9b-51e0-47d9-897c-fb23bcdd0750"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "---------------\n",
            "Certainly! The content you've provided appears to be a vertically arranged\n",
            "sequence of letters that spells out \"define a rag store\" when read horizontally.\n",
            "Let's break down and elaborate on this phrase:  1. **Define**: This is a verb\n",
            "that means to explain the meaning of a word or concept. In this context, it\n",
            "suggests that we are being asked to provide a clear explanation or description\n",
            "of what a \"rag store\" is.  2. **A**: This is an indefinite article used in\n",
            "English to refer to a non-specific item or concept. It precedes a noun and\n",
            "indicates that the noun is singular and not previously specified.  3. **Rag\n",
            "Store**: This term can be interpreted in a couple of ways:    - **Literal\n",
            "Interpretation**: A \"rag store\" could refer to a shop or store that sells rags.\n",
            "Rags are typically pieces of old cloth used for cleaning or other purposes. Such\n",
            "stores might sell cleaning supplies, including various types of cloths and rags.\n",
            "- **Metaphorical or Colloquial Interpretation**: In some contexts, \"rag store\"\n",
            "might be used colloquially to describe a second-hand clothing store or a thrift\n",
            "shop. These are places where used clothing and other items are sold at a lower\n",
            "price. The term \"rag\" in this sense might refer to clothing that is considered\n",
            "old or worn.  In summary, \"define a rag store\" is a request to explain what a\n",
            "rag store is, which could be a shop selling cleaning rags or a second-hand\n",
            "clothing store, depending on the context.\n",
            "---------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# calculate cosine similarity between the query and each record of the dataset\n",
        "def calculate_cosine_similarity(text1, text2):\n",
        "  vectorizer = TfidfVectorizer(\n",
        "      stop_words='english', # Ignores common English words\n",
        "      use_idf=True, # Enables inverse document frequency weighting\n",
        "      ngram_range=(1,2), # Considers both single words and two-word combinations\n",
        "      norm='l2', # apply L2 normalization\n",
        "      sublinear_tf=True, # Applies log term frequency scaling\n",
        "      analyzer='word' # Analyzes text at the word level\n",
        "  )\n",
        "  tfidf = vectorizer.fit_transform([text1, text2])\n",
        "  similarity = cosine_similarity(tfidf[0:1], tfidf[1:2])\n",
        "  return similarity[0][0]\n"
      ],
      "metadata": {
        "id": "KpX6iHGCaU-w"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enhanced similarity\n",
        "import spacy\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "# Load spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def get_synonyms(word):\n",
        "  synonyms = set()\n",
        "  for syn in wordnet.synsets(word):\n",
        "    for lemma in syn.lemmas():\n",
        "      synonyms.add(lemma.name())\n",
        "  return synonyms\n",
        "\n",
        "def preprocess_text(text):\n",
        "  doc = nlp(text.lower())\n",
        "  lemmatized_words = []\n",
        "  for token in doc:\n",
        "    if token.is_stop or token.is_punct:\n",
        "      continue\n",
        "    lemmatized_words.append(token.lemma_)\n",
        "  return lemmatized_words\n",
        "\n",
        "def expand_with_synonyms(words):\n",
        "  expanded_words = words.copy()\n",
        "  for word in words:\n",
        "    expanded_words.extend(get_synonyms(word))\n",
        "  return expanded_words\n",
        "\n",
        "def calculate_enhanced_similarity(text1, text2):\n",
        "  # Preprocess and tokenize texts\n",
        "  words1 = preprocess_text(text1)\n",
        "  words2 = preprocess_text(text2)\n",
        "\n",
        "  # Expand text with synonyms\n",
        "  expanded_words1 = expand_with_synonyms(words1)\n",
        "  expanded_words2 = expand_with_synonyms(words2)\n",
        "\n",
        "  # Count word frequencies\n",
        "  freq1 = Counter(expanded_words1)\n",
        "  freq2 = Counter(expanded_words2)\n",
        "\n",
        "  # Create a set of all unique words\n",
        "  unique_words = set(freq1.keys()).union(set(freq2.keys()))\n",
        "\n",
        "  # Create frequency vectors\n",
        "  vector1 = [freq1[word] for word in unique_words]\n",
        "  vector2 = [freq2[word] for word in unique_words]\n",
        "\n",
        "  # Convert lists to numpy arrays\n",
        "  vector1 = np.array(vector1)\n",
        "  vector2 = np.array(vector2)\n",
        "\n",
        "  # Calculate cosine similarity\n",
        "  cosine_similarity = np.dot(vector1, vector2) / (np.linalg.norm(vector1) * np.linalg.norm(vector2))\n",
        "\n",
        "  return cosine_similarity\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkJLQR5VbksB",
        "outputId": "a83849b6-1c84-4b6d-8bec-093e8f876580"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "/usr/local/lib/python3.11/dist-packages/spacy/util.py:1740: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n",
            "  warnings.warn(Warnings.W111)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Naive RAG"
      ],
      "metadata": {
        "id": "HsdLgPvYfYZr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# keyword search and matching\n",
        "def find_best_match_keyword_search(query, db_records):\n",
        "  best_score = 0\n",
        "  best_record = None\n",
        "\n",
        "  # Split the query into words\n",
        "  query_keywords = set(query.lower().split())\n",
        "\n",
        "  # Iterate through each record in db_records\n",
        "  for record in db_records:\n",
        "    # Split the record into keywords\n",
        "    record_keywords = set(record.lower().split())\n",
        "\n",
        "    # Calculate the number of common keywords\n",
        "    common_keywords = query_keywords.intersection(record_keywords)\n",
        "    current_score = len(common_keywords)\n",
        "\n",
        "    # Update best score if current score is higher\n",
        "    if current_score > best_score:\n",
        "      best_score = current_score\n",
        "      best_record = record\n",
        "\n",
        "    return best_score, best_record"
      ],
      "metadata": {
        "id": "A-94dvRPfaUn"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_keyword_score, best_matching_record = find_best_match_keyword_search(query, db_records)\n",
        "\n",
        "print(f\"Best Keyword Score: {best_keyword_score}\")\n",
        "print_formatted_response(best_matching_record)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OYOie2Fmgi5x",
        "outputId": "a97565a5-715f-4a8e-af16-baf01d740c78"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Keyword Score: 1\n",
            "Response:\n",
            "---------------\n",
            "Retrieval Augmented Generation (RAG) represents a sophisticated hybrid approach\n",
            "in the field of artificial intelligence, particularly within the realm of\n",
            "natural language processing (NLP).\n",
            "---------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cosine similarity\n",
        "score = calculate_cosine_similarity(query, best_matching_record)\n",
        "print(score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VWJrPUG4gjfg",
        "outputId": "79c974bb-0dbf-467e-f23b-d6e1f02d76f8"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.04182819438170016\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Enhanced similarity\n",
        "response = best_matching_record\n",
        "print(query, \": \", response)\n",
        "\n",
        "similarity_score = calculate_enhanced_similarity(query, response)\n",
        "print(similarity_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OG4DYpUfgsIw",
        "outputId": "de445487-b6cd-4db3-fce9-e27b9475e2c6"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "define a rag store :  Retrieval Augmented Generation (RAG) represents a sophisticated hybrid approach in the field of artificial intelligence, particularly within the realm of natural language processing (NLP).\n",
            "0.37929771266843937\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Augmented input\n",
        "augmented_input = query + \": \" + best_matching_record\n",
        "print_formatted_response(augmented_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eacAIDCeg50n",
        "outputId": "5cc694d2-aa46-4de1-cc54-c446f3df3255"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "---------------\n",
            "define a rag store: Retrieval Augmented Generation (RAG) represents a\n",
            "sophisticated hybrid approach in the field of artificial intelligence,\n",
            "particularly within the realm of natural language processing (NLP).\n",
            "---------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the function and print the result\n",
        "llm_response = call_llm_with_full_text(augmented_input)\n",
        "print_formatted_response(llm_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_DXwvrKhEIQ",
        "outputId": "b1287ca7-4102-4b7b-bfc3-456fe173de7e"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "---------------\n",
            "Retrieval Augmented Generation (RAG) is an advanced technique in artificial\n",
            "intelligence, specifically within the domain of natural language processing\n",
            "(NLP). It combines the strengths of two distinct approaches: retrieval-based\n",
            "methods and generative models. Here's a detailed explanation of how RAG works\n",
            "and its significance:  1. **Retrieval-Based Methods**: These methods involve\n",
            "searching through a large corpus of text to find relevant information or\n",
            "documents that can help answer a query. The retrieval process is typically based\n",
            "on similarity measures, such as cosine similarity or other distance metrics, to\n",
            "identify the most relevant pieces of information. This approach is efficient for\n",
            "tasks where the answer is likely to be found in existing data, such as question-\n",
            "answering systems or information retrieval tasks.  2. **Generative Models**:\n",
            "These models, often based on neural networks like transformers, are capable of\n",
            "generating new text based on a given input. They are trained on large datasets\n",
            "to learn the patterns and structures of language, enabling them to produce\n",
            "coherent and contextually relevant text. Generative models are powerful for\n",
            "tasks that require creativity or the synthesis of new information, such as text\n",
            "completion or dialogue generation.  3. **Hybrid Approach**: RAG combines these\n",
            "two approaches to leverage their respective strengths. In a typical RAG system,\n",
            "when a query is presented, the system first uses the retrieval component to find\n",
            "relevant documents or passages from a large corpus. These retrieved documents\n",
            "provide context and factual grounding for the task at hand.  4. **Augmented\n",
            "Generation**: Once the relevant information is retrieved, the generative model\n",
            "uses this information as additional context to produce a more accurate and\n",
            "contextually relevant response. This augmentation allows the generative model to\n",
            "incorporate factual data from the retrieval step, improving the quality and\n",
            "reliability of the generated output.  5. **Applications**: RAG is particularly\n",
            "useful in scenarios where both factual accuracy and natural language generation\n",
            "are important. For example, in customer support chatbots, RAG can retrieve\n",
            "relevant knowledge base articles and generate responses that are both\n",
            "informative and conversational. It is also used in complex question-answering\n",
            "systems, content creation, and summarization tasks.  6. **Advantages**: The main\n",
            "advantage of RAG is its ability to produce responses that are both contextually\n",
            "rich and factually accurate. By grounding the generative process in real-world\n",
            "data, RAG reduces the risk of generating incorrect or nonsensical information, a\n",
            "common challenge in purely generative models.  In summary, Retrieval Augmented\n",
            "Generation represents a sophisticated and effective approach in NLP by combining\n",
            "the precision of retrieval methods with the flexibility of generative models.\n",
            "This hybrid technique enhances the capabilities of AI systems in understanding\n",
            "and generating human-like text, making it a valuable tool in various\n",
            "applications across industries.\n",
            "---------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Advanced RAG"
      ],
      "metadata": {
        "id": "BKNHW7ENi4YD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vector search"
      ],
      "metadata": {
        "id": "G_MSEY4OljYJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_best_match(text_input, records):\n",
        "  best_score = 0\n",
        "  best_record = None\n",
        "\n",
        "  for record in records:\n",
        "    current_score = calculate_cosine_similarity(text_input, record)\n",
        "\n",
        "    if current_score > best_score:\n",
        "      best_score = current_score\n",
        "      best_record = record\n",
        "\n",
        "    return best_score, best_record"
      ],
      "metadata": {
        "id": "dWf1Ypyqi5_Y"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_similarity_score, best_matching_record = find_best_match(query, db_records)\n",
        "print_formatted_response(best_matching_record)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hjqAlXaikHZw",
        "outputId": "1630c637-3056-4848-f5d3-589f1ce937a7"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "---------------\n",
            "Retrieval Augmented Generation (RAG) represents a sophisticated hybrid approach\n",
            "in the field of artificial intelligence, particularly within the realm of\n",
            "natural language processing (NLP).\n",
            "---------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Best Cosine Similarity Score: {best_similarity_score:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mdl6QVNykXn6",
        "outputId": "cf6b3fa7-e183-434f-82bb-02e522331792"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Cosine Similarity Score: 0.042\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Enhanced Similarity\n",
        "response = best_matching_record\n",
        "print(query,\": \", response)\n",
        "similarity_score = calculate_enhanced_similarity(query, best_matching_record)\n",
        "print(f\"Enhanced Similarity:, {similarity_score:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_hTD3GgkonB",
        "outputId": "c13d72e6-7a03-4750-cfa8-108a54103ba9"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "define a rag store :  Retrieval Augmented Generation (RAG) represents a sophisticated hybrid approach in the field of artificial intelligence, particularly within the realm of natural language processing (NLP).\n",
            "Enhanced Similarity:, 0.379\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the function and print the result\n",
        "augmented_input=query+\": \"+best_matching_record\n",
        "print_formatted_response(augmented_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3qv-o07kkxIB",
        "outputId": "2ed1c5ac-8cf2-41e7-f922-f51eff50d2eb"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "---------------\n",
            "define a rag store: Retrieval Augmented Generation (RAG) represents a\n",
            "sophisticated hybrid approach in the field of artificial intelligence,\n",
            "particularly within the realm of natural language processing (NLP).\n",
            "---------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the function and print the result\n",
        "augmented_input=query+best_matching_record\n",
        "llm_response = call_llm_with_full_text(augmented_input)\n",
        "print_formatted_response(llm_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVR6Ek00k84Z",
        "outputId": "a0cf3451-5224-4c49-9fb1-48d4a221ba0a"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "---------------\n",
            "Retrieval-Augmented Generation (RAG) is an advanced technique in the field of\n",
            "artificial intelligence, specifically within natural language processing (NLP).\n",
            "It represents a hybrid approach that combines the strengths of both retrieval-\n",
            "based and generation-based models to enhance the performance of AI systems in\n",
            "understanding and generating human language.  Here's a detailed breakdown of the\n",
            "components and benefits of RAG:  1. **Retrieval Component**:     - The retrieval\n",
            "part of RAG involves searching through a large corpus of documents or data to\n",
            "find relevant information that can aid in answering a query or completing a\n",
            "task. This is akin to how search engines work, where the system retrieves the\n",
            "most pertinent documents based on the input query.    - This component ensures\n",
            "that the model has access to a vast amount of external knowledge, which can be\n",
            "particularly useful for tasks requiring up-to-date or domain-specific\n",
            "information.  2. **Generation Component**:    - The generation aspect involves\n",
            "using a language model to produce coherent and contextually appropriate text\n",
            "based on the input and the retrieved information. This is similar to how models\n",
            "like GPT (Generative Pre-trained Transformer) generate text.    - The generation\n",
            "component allows the system to create responses that are not only relevant but\n",
            "also fluent and natural-sounding.  3. **Hybrid Approach**:    - By combining\n",
            "retrieval and generation, RAG leverages the strengths of both methods. The\n",
            "retrieval component provides a strong foundation of factual and relevant\n",
            "information, while the generation component ensures that the output is well-\n",
            "formed and contextually appropriate.    - This hybrid approach addresses some of\n",
            "the limitations of standalone retrieval or generation models. For instance,\n",
            "retrieval models might struggle with generating coherent text, while generation\n",
            "models might produce plausible but factually incorrect information.  4.\n",
            "**Applications in NLP**:    - RAG is particularly useful in applications such as\n",
            "question answering, where the system needs to provide accurate and contextually\n",
            "relevant answers to user queries.    - It can also be applied in tasks like\n",
            "summarization, dialogue systems, and any other NLP task that benefits from both\n",
            "factual accuracy and natural language generation.  5. **Advantages**:    -\n",
            "**Improved Accuracy**: By grounding the generation process in retrieved\n",
            "information, RAG can produce more accurate and reliable outputs.    -\n",
            "**Flexibility**: The model can adapt to various domains by simply changing the\n",
            "corpus from which it retrieves information, making it versatile for different\n",
            "applications.    - **Scalability**: As the retrieval component can access vast\n",
            "amounts of data, RAG systems can scale to handle large and diverse datasets.  In\n",
            "summary, Retrieval-Augmented Generation (RAG) is a sophisticated approach in NLP\n",
            "that enhances the capabilities of AI systems by integrating retrieval and\n",
            "generation processes, leading to more accurate, relevant, and natural language\n",
            "outputs.\n",
            "---------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Index based search"
      ],
      "metadata": {
        "id": "BP2-wpTDleAh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def setup_vectorizer(records):\n",
        "  vectorizer = TfidfVectorizer()\n",
        "  tfidf_matrix = vectorizer.fit_transform(records)\n",
        "  return vectorizer, tfidf_matrix\n",
        "\n",
        "def find_best_match(query, vectorizer, tfidf_matrix):\n",
        "  query_tfidf = vectorizer.transform([query])\n",
        "  similarities = cosine_similarity(query_tfidf, tfidf_matrix)\n",
        "\n",
        "  best_index = similarities.argmax() # Index with highest similarity score\n",
        "  best_score = similarities[0, best_index]\n",
        "\n",
        "  return best_score, best_index\n",
        "\n",
        "vectorizer, tfidf_matrix = setup_vectorizer(db_records)\n",
        "\n",
        "best_similarity_score, best_index = find_best_match(query, vectorizer, tfidf_matrix)\n",
        "best_matching_record = db_records[best_index]\n",
        "\n",
        "print_formatted_response(best_matching_record)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbvYJDNWlf1J",
        "outputId": "d666f59c-5362-457f-fabf-239497afea0a"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "---------------\n",
            "A RAG vector store is a database or dataset that contains vectorized data\n",
            "points.\n",
            "---------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def setup_vectorizer(records):\n",
        "  vectorizer = TfidfVectorizer()\n",
        "  tfidf_matrix = vectorizer.fit_transform(records)\n",
        "\n",
        "  # Convert the TF-IDF matrix to a DataFrame for display purposes\n",
        "  tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "\n",
        "  # Display the DataFrame\n",
        "  print(tfidf_df)\n",
        "\n",
        "  return vectorizer, tfidf_matrix\n",
        "\n",
        "vectorizer, tfidf_matrix = setup_vectorizer(db_records)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60d608C5qc00",
        "outputId": "94eeb935-2414-447d-f021-090c6ffd22bb"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     ability    access  accuracy  accurate     adapt  additional  advancement  \\\n",
            "0   0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "1   0.000000  0.000000  0.000000  0.216364  0.000000    0.000000     0.000000   \n",
            "2   0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "3   0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "4   0.000000  0.000000  0.000000  0.000000  0.000000    0.236479     0.000000   \n",
            "5   0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "6   0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "7   0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "8   0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "9   0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "10  0.186734  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "11  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "12  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "13  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "14  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "15  0.000000  0.172624  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "16  0.000000  0.317970  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "17  0.000000  0.000000  0.000000  0.206861  0.000000    0.000000     0.000000   \n",
            "18  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "19  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "20  0.000000  0.000000  0.000000  0.000000  0.275802    0.000000     0.000000   \n",
            "21  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "22  0.000000  0.174772  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "23  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "24  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "25  0.000000  0.000000  0.228743  0.000000  0.000000    0.000000     0.000000   \n",
            "26  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.173327   \n",
            "27  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "\n",
            "      advice  algorithms    allows  ...    vector  vectorized      when  \\\n",
            "0   0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "1   0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "2   0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "3   0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "4   0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "5   0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "6   0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "7   0.000000    0.220687  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "8   0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "9   0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "10  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "11  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.295573   \n",
            "12  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "13  0.000000    0.000000  0.000000  ...  0.200131     0.00000  0.000000   \n",
            "14  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "15  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "16  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "17  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "18  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "19  0.244401    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "20  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "21  0.000000    0.000000  0.291503  ...  0.000000     0.00000  0.000000   \n",
            "22  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "23  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "24  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "25  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "26  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "27  0.000000    0.000000  0.000000  ...  0.307719     0.34589  0.000000   \n",
            "\n",
            "       where     which    while     wide      with    within   without  \n",
            "0   0.000000  0.000000  0.00000  0.00000  0.000000  0.260582  0.000000  \n",
            "1   0.000000  0.000000  0.00000  0.00000  0.160278  0.000000  0.000000  \n",
            "2   0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "3   0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "4   0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "5   0.000000  0.247710  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "6   0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "7   0.000000  0.179053  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "8   0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "9   0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "10  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "11  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "12  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "13  0.000000  0.182517  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "14  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "15  0.189283  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "16  0.000000  0.000000  0.00000  0.00000  0.258278  0.000000  0.000000  \n",
            "17  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "18  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "19  0.217430  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "20  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "21  0.000000  0.000000  0.00000  0.00000  0.192110  0.000000  0.291503  \n",
            "22  0.000000  0.000000  0.00000  0.21541  0.141963  0.000000  0.000000  \n",
            "23  0.000000  0.000000  0.32932  0.00000  0.217033  0.000000  0.000000  \n",
            "24  0.000000  0.000000  0.00000  0.00000  0.134513  0.000000  0.000000  \n",
            "25  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "26  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "27  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "\n",
            "[28 rows x 297 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "augmented_input = query + \": \" + best_matching_record\n",
        "\n",
        "print_formatted_response(augmented_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3L48G63Xr4d-",
        "outputId": "3d4533cf-da0f-4b89-b19f-4322aa3be3df"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "---------------\n",
            "define a rag store: A RAG vector store is a database or dataset that contains\n",
            "vectorized data points.\n",
            "---------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the function and print the result\n",
        "llm_response = call_llm_with_full_text(augmented_input)\n",
        "print_formatted_response(llm_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DS5JV6SlsGj-",
        "outputId": "95eaa8b8-2189-45bc-8e75-bfcd771cebf4"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "---------------\n",
            "An \"ARAG vector store\" refers to a specialized type of database or dataset\n",
            "designed to store and manage vectorized data points. Let's break down the\n",
            "concept further:  1. **Vectorized Data Points**: In the context of data science\n",
            "and machine learning, vectorization is the process of converting data into a\n",
            "numerical format that algorithms can easily process. Each data point is\n",
            "represented as a vector, which is essentially an array of numbers. These vectors\n",
            "can represent various types of data, such as text, images, or any other form of\n",
            "information that can be numerically encoded.  2. **Purpose of a Vector Store**:\n",
            "The primary purpose of a vector store is to efficiently store and retrieve these\n",
            "vectorized data points. This is crucial for tasks that involve similarity\n",
            "searches, clustering, or any operation that requires comparing data points based\n",
            "on their numerical representations.  3. **Applications**: Vector stores are\n",
            "commonly used in applications such as recommendation systems, natural language\n",
            "processing, and computer vision. For example, in a recommendation system, user\n",
            "preferences and item characteristics can be vectorized, and the vector store can\n",
            "be queried to find items similar to those a user has liked in the past.  4.\n",
            "**ARAG Specifics**: While the term \"ARAG\" isn't widely recognized in the context\n",
            "of vector stores, it could refer to a specific implementation or framework\n",
            "designed for handling vectorized data. It might include optimizations or\n",
            "features tailored to particular use cases or industries.  5. **Benefits**: Using\n",
            "a vector store allows for efficient data retrieval and manipulation, especially\n",
            "when dealing with large datasets. It supports operations like nearest neighbor\n",
            "search, which is essential for finding similar items or data points quickly.  In\n",
            "summary, an ARAG vector store is a system designed to handle and manage\n",
            "vectorized data efficiently, enabling advanced data processing and retrieval\n",
            "tasks in various applications.\n",
            "---------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modular RAG"
      ],
      "metadata": {
        "id": "hMqZ4sqKs0aE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "class RetrievalComponent:\n",
        "    def __init__(self, method='vector'):\n",
        "        self.method = method\n",
        "        if self.method == 'vector' or self.method == 'indexed':\n",
        "            self.vectorizer = TfidfVectorizer()\n",
        "            self.tfidf_matrix = None\n",
        "\n",
        "    def fit(self, records):\n",
        "      self.documents = records  # Initialize self.documents here\n",
        "      if self.method == 'vector' or self.method == 'indexed':\n",
        "        self.tfidf_matrix = self.vectorizer.fit_transform(records)\n",
        "\n",
        "    def retrieve(self, query):\n",
        "        if self.method == 'keyword':\n",
        "            return self.keyword_search(query)\n",
        "        elif self.method == 'vector':\n",
        "            return self.vector_search(query)\n",
        "        elif self.method == 'indexed':\n",
        "            return self.indexed_search(query)\n",
        "\n",
        "    def keyword_search(self, query):\n",
        "        best_score = 0\n",
        "        best_record = None\n",
        "        query_keywords = set(query.lower().split())\n",
        "        for index, doc in enumerate(self.documents):\n",
        "            doc_keywords = set(doc.lower().split())\n",
        "            common_keywords = query_keywords.intersection(doc_keywords)\n",
        "            score = len(common_keywords)\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best_record = self.documents[index]\n",
        "        return best_record\n",
        "\n",
        "    def vector_search(self, query):\n",
        "        query_tfidf = self.vectorizer.transform([query])\n",
        "        similarities = cosine_similarity(query_tfidf, self.tfidf_matrix)\n",
        "        best_index = similarities.argmax()\n",
        "        return db_records[best_index]\n",
        "\n",
        "    def indexed_search(self, query):\n",
        "        # Assuming the tfidf_matrix is precomputed and stored\n",
        "        query_tfidf = self.vectorizer.transform([query])\n",
        "        similarities = cosine_similarity(query_tfidf, self.tfidf_matrix)\n",
        "        best_index = similarities.argmax()\n",
        "        return db_records[best_index]"
      ],
      "metadata": {
        "id": "rh1Kf43Rsrxb"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Usage example\n",
        "retrieval = RetrievalComponent(method='indexed')  # Choose from 'keyword', 'vector', 'indexed'\n",
        "retrieval.fit(db_records)\n",
        "best_matching_record = retrieval.retrieve(query)\n",
        "\n",
        "print_formatted_response(best_matching_record)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3Ge2pQ7u2QL",
        "outputId": "735ebcb7-f352-42d2-f3b5-d94a9dc1489b"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "---------------\n",
            "A RAG vector store is a database or dataset that contains vectorized data\n",
            "points.\n",
            "---------------\n",
            "\n"
          ]
        }
      ]
    }
  ]
}